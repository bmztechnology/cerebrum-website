# Support Vector Machines (SVM): The Hyperplane

## I. Executive Summary

**Support Vector Machines** are a powerful supervised learning algorithm for classification (Up/Down) and regression. Unlike Neural Nets which minimize error, SVM minimizes **Generalization Risk** by finding the "Maximum Margin Hyperplane" that separates the classes. This makes SVMs incredibly robust to overfitting in high-dimensional financial spaces (the "Kernel Trick").

## II. Formal Definitions

### Definition 2.1 (The Margin)
Distance between the separating hyperplane and the nearest data points (Support Vectors).
Maximize $\frac{2}{||w||}$.
Subject to $y_i (w \cdot x_i + b) \ge 1$.

### Definition 2.2 (The Kernel Trick)
If data is not linearly separable (e.g., XOR problem), project it into infinite dimensional space.
$K(x_i, x_j) = \phi(x_i) \cdot \phi(x_j)$.
Common Kernels: RBF (Gaussian), Polynomial.

## III. Theoretical Framework

### 3.1 Structural Risk Minimization (Vapnik)
SVM does not care about "correctly classifying" the easy points far from the boundary.
It cares *only* about the difficult points (Support Vectors) near the boundary.
In finance, these are the regime-transition days.
Because it ignores the bulk of data (easy days), it is stable.

### 3.2 Support Vector Regression (SVR)
Instead of a line, find a "Tube" of width $\epsilon$ around the price.
Points inside the tube are ignored (Zero Loss).
Points outside contribute linear penalty.
Ideal for volatility filtering.

## IV. Strategic Applications

### 4.1 Directional Forecasting
Input: RSI, MACD, Returns.
Target: Up/Down tomorrow.
SVM with RBF Kernel often outperforms Logistic Regression because it captures non-linear interactions (e.g., RSI is only bearish if MACD is also bearish).

### 4.2 Outlier Detection (One-Class SVM)
Train on "Normal" market data.
If new data falls on the "wrong" side of the hyperplane, it is an Anomaly.
Use for "Kill Switch" logic. Detection of Flash Crash mechanics.

## V. Exercises

**Exercise 1 (Logic):**
Why is SVM better than Neural Net for small data?
(SVM is a convex optimization problem [Quadratic Programming]. It has a unique global minimum. NN has local minima. With small data, NN overfits; SVM finds the specific max-margin solution).

**Exercise 2 (Parameters):**
$C$ parameter.
High $C$: Strict classification (Hard Margin). Overfitting risk.
Low $C$: Allow errors (Soft Margin). Smoother boundary.
In noisy finance, use Low $C$.

## VI. References
-   Vapnik, V. *The Nature of Statistical Learning Theory*.
-   Cristianini, N. *An Introduction to Support Vector Machines*.
